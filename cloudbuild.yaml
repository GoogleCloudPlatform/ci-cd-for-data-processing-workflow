# Copyright 2019 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
steps:
# Run linters 
- name: 'gcr.io/datapipelines-ci/make'
  args: ['test'] 
  is: 'run-sytle-and-unit-tests'
# [Dataflow]
# Build JARs.
- name: gcr.io/cloud-builders/mvn
  args: ['package', '-q']
  dir: './dataflow/java/wordcount/'
  id: 'build-word-count-jar'
# Stage JARs on GCS.
- name: gcr.io/cloud-builders/gsutil
  args: ['cp', '*bundled*.jar', 'gs://${_DATAFLOW_JAR_BUCKET}/dataflow_deployment_$BUILD_ID.jar']
  dir: './dataflow/java/wordcount/target'
  id: 'deploy-jar'
# [BigQuery]
# Dry Run SQL.
- name: gcr.io/cloud-builders/gcloud
  entrypoint: 'bash'
  args: ['tests/test_sql.sh', 'sql/']
  dir: './bigquery'
  id: 'test-sql-queries'
# Copy SQL to DAGs folder.
- name: gcr.io/cloud-builders/gsutil
  args: ['rsync','-r', '-d', 'sql', '${_COMPOSER_DAG_BUCKET}dags/sql']
  dir: './bigquery/'
  id: 'deploy-sql-queries-for-composer'
# [Composer]
# Render AirflowVariables.json
- name: 'gcr.io/${PROJECT_ID}/envsubst'
  env: [
          "GCP_PROJECT_ID=${PROJECT_ID}",
          "COMPOSTER_REGION=${_COMPOSER_REGION}",
          "DATAFLOW_JAR_BUCKET=${_DATAFLOW_JAR_BUCKET}",
          "INPUT_BUCKET=${_WORDCOUNT_INPUT_BUCKET}",
          "REF_BUCKET=${_WORDCOUNT_REF_BUCKET}",
          "RESULT_BUCKET=${_WORDCOUNT_RESULT_BUCKET}",
          "DATAFLOW_STAGING_BUCKET=${_DATAFLOW_STAGING_BUCKET}",
       ] 
  args: ['AirflowVariables.json']
  dir: './composer/config'
  id: 'render-airflow-variables'
# Run unit tests in Airflow container (local to cloud build).
- name: 'gcr.io/cloud-solutions-images/apache-airflow:1.10' 
  entrypoint: 'bash'
  args: ['cloudbuild/bin/run_tests.sh']
  dir: './composer/'
  id: 'run-unit-tests'
# Add .airflowignore to GCS DAGs folder.
- name: gcr.io/cloud-builders/gcloud
  args: [
          'composer', 'environments', 'storage', 'dags', 'import',
          '--source','.airflowignore',
          '--environment', '${_COMPOSER_ENV_NAME}',
          '--location', '${_COMPOSER_REGION}'
        ]
  dir: './composer/dags/'
  id: 'deploy-airflowignore'
# Stage files for running the example.
- name: gcr.io/cloud-builders/gsutil
  args: ['cp', 'support-files/input.txt', 'gs://${_WORDCOUNT_INPUT_BUCKET}']
  dir: './composer/dags'
  id: 'deploy-test-input-file'
- name: gcr.io/cloud-builders/gsutil
  args: ['cp', 'support-files/ref.txt', 'gs://${_WORDCOUNT_REF_BUCKET}']
  dir: './composer/dags'
  id: 'deploy-test-ref-file'
# Stage AirflowVariables.json to data directory to be synced to workers.
- name: gcr.io/cloud-builders/gcloud
  args: [
          'composer', 'environments', 'storage', 'data', 'import',
          '--location=${_COMPOSER_REGION}',
          '--environment=${_COMPOSER_ENV_NAME}',
          '--source','AirflowVariables.json',
          '--destination', 'config'
        ]
  dir: './composer/config/'
  id: 'stage-airflow-variables'
# Import AirflowVariables.json 
- name: gcr.io/cloud-builders/gcloud
  args: [
          'composer', 'environments', 'run', 
          '--location=${_COMPOSER_REGION}',
          '${_COMPOSER_ENV_NAME}',
          'variables', '--',
          '--import', '/home/airflow/gcs/data/config/AirflowVariables.json'
        ] 
  id: 'import-airflow-variables'
# Override JAR reference variable to the artifact built in this build.
- name: gcr.io/cloud-builders/gcloud
  args: [
          'composer', 'environments', 'run',
          '--location', '${_COMPOSER_REGION}',
          '${_COMPOSER_ENV_NAME}', 
          'variables', '--', 
          '--set', 'dataflow_jar_file_test', 'dataflow_deployment_$BUILD_ID.jar'
        ]
  id: 'set-composer-test-jar-ref'
# Sync plugins to GCS plugins dir
- name: gcr.io/cloud-builders/gsutil
  args: ['rsync','-r', '-d', 'plugins/', '${_COMPOSER_DAG_BUCKET}plugins']
  dir: './composer/'
  id: 'deploy-custom-plugins'
# Sync DAGs to data dir for integration test parsing in  target Composer Environment.
- name: gcr.io/cloud-builders/gsutil
  args: ['rsync','-r', '-d', 'dags/', '${_COMPOSER_DAG_BUCKET}data/test-dags/$BUILD_ID']
  dir: './composer/'
  id: 'stage-for-integration-test'
# Run integration tests parsing in target Composer Environment.
- name: gcr.io/cloud-builders/gcloud
  args: [
          'composer', 'environments', 'run', 
          '--location', '${_COMPOSER_REGION}',
          '${_COMPOSER_ENV_NAME}', 
          'list_dags', '--', 
          '-sd', '/home/airflow/gcs/data/test-dags/$BUILD_ID'
        ] 
  id: 'dag-parse-integration-test'
# Clean up. 
- name: gcr.io/cloud-builders/gsutil
  args: ['-m', 'rm','-r', '${_COMPOSER_DAG_BUCKET}data/test-dags/$BUILD_ID']
  dir: './composer/'
  id: 'clean-up-data-dir-dags'
# pull dags deployer golang app.
- name: gcr.io/cloud-builders/docker
  entrypoint: 'bash'
  args:
   - '-c'
   - |
     docker pull gcr.io/$PROJECT_ID/deploydags:latest || exit 0
# build with cache
- name: gcr.io/cloud-builders/docker
  dir: './composer/cloudbuild/go/dagsdeployer'
  args: [
          'build', 
          '-t', 'gcr.io/${PROJECT_ID}/deploydags:latest',
          '--cache-from', 'gcr.io/${PROJECT_ID}/deploydags:latest',
          '.'
        ]
  id: 'build-deploydags'
# Run dags deployer golang app. 
- name: gcr.io/${PROJECT_ID}/deploydags
  dir: './composer'
  args: [
          '-dagList=./config/running_dags.txt',
          '-dagsFolder=./dags',
          '-project=${PROJECT_ID}',
          '-region=${_COMPOSER_REGION}',
          '-composerEnv=${_COMPOSER_ENV_NAME}',
          '-dagBucketPrefix=${_COMPOSER_DAG_BUCKET}dags',
          '-replace'
        ]
  id: 'run-deploydags'
options:
  machineType: 'N1_HIGHCPU_8'
